{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Finetuning Transformer for Sentiment Analysis\nObjective: Finetune a BERT model on SST2 dataset to perform binary sentiment analysis\n\n### Dataset\n\nStanford Sentiment Treebank v2: set of 67349 phrases and human-labeled sentiment class\n\n### Model\nChosen to finetune distilbert base uncased (67M parameters) based on smaller size and speed of training","metadata":{}},{"cell_type":"code","source":"!pip install -q datasets transformers accelerate evaluate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-29T01:05:28.309343Z","iopub.execute_input":"2025-12-29T01:05:28.310065Z","iopub.status.idle":"2025-12-29T01:05:34.624932Z","shell.execute_reply.started":"2025-12-29T01:05:28.310030Z","shell.execute_reply":"2025-12-29T01:05:34.624205Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():\n    num_gpus = torch.cuda.device_count()\n    print(f\"✓ Number of GPUs: {num_gpus}\")\n    for i in range(num_gpus):\n        gpu_name = torch.cuda.get_device_name(i)\n        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1e9\n        print(f\"  GPU {i}: {gpu_name} ({gpu_memory:.2f} GB)\")\n    device = torch.device(\"cuda\")\nelse:\n    print(\"⚠ No GPU available, using CPU\")\n    device = torch.device(\"cpu\")\n\n# Clear GPU cache\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T01:05:34.626338Z","iopub.execute_input":"2025-12-29T01:05:34.626618Z","iopub.status.idle":"2025-12-29T01:05:41.646779Z","shell.execute_reply.started":"2025-12-29T01:05:34.626588Z","shell.execute_reply":"2025-12-29T01:05:41.646063Z"}},"outputs":[{"name":"stdout","text":"✓ Number of GPUs: 2\n  GPU 0: Tesla T4 (15.83 GB)\n  GPU 1: Tesla T4 (15.83 GB)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import numpy as np\nimport evaluate\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    DataCollatorWithPadding,\n    TrainingArguments,\n    Trainer,\n    EarlyStoppingCallback\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T01:05:41.647622Z","iopub.execute_input":"2025-12-29T01:05:41.648027Z","iopub.status.idle":"2025-12-29T01:06:25.448277Z","shell.execute_reply.started":"2025-12-29T01:05:41.647989Z","shell.execute_reply":"2025-12-29T01:06:25.447689Z"}},"outputs":[{"name":"stderr","text":"2025-12-29 01:05:59.321386: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766970359.765534      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766970359.881425      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766970361.024894      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766970361.024936      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766970361.024939      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766970361.024942      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Set to True for quick pipeline testing, False for full training\nTEST_MODE = False\nTEST_SUBSET_SIZE = 1000\n\ncheckpoint = \"distilbert-base-uncased\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T01:06:25.449690Z","iopub.execute_input":"2025-12-29T01:06:25.450290Z","iopub.status.idle":"2025-12-29T01:06:25.453864Z","shell.execute_reply.started":"2025-12-29T01:06:25.450265Z","shell.execute_reply":"2025-12-29T01:06:25.453082Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"dataset = load_dataset(\"stanfordnlp/sst2\")\n\nprint(\"Original dataset structure:\")\nprint(dataset)\n\n# Create a proper test set from training data since official test labels are hidden\ntrain_valid_test = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\ntrain_valid = train_valid_test[\"train\"].train_test_split(test_size=0.1, seed=42)\n\ndataset[\"train\"] = train_valid[\"train\"]\ndataset[\"validation\"] = train_valid[\"test\"]  # Use as validation\ndataset[\"test\"] = train_valid_test[\"test\"]   # Use as test\n\nprint(\"\\nModified dataset structure:\")\nprint(f\"  Train: {len(dataset['train'])} samples\")\nprint(f\"  Validation: {len(dataset['validation'])} samples\")\nprint(f\"  Test: {len(dataset['test'])} samples\")\n\nif TEST_MODE:\n    print(f\"\\n⚠ TEST MODE: Using {TEST_SUBSET_SIZE} samples per split\")\n    dataset[\"train\"] = dataset[\"train\"].shuffle(seed=42).select(range(min(TEST_SUBSET_SIZE, len(dataset[\"train\"]))))\n    dataset[\"validation\"] = dataset[\"validation\"].shuffle(seed=42).select(range(min(TEST_SUBSET_SIZE, len(dataset[\"validation\"]))))\n    dataset[\"test\"] = dataset[\"test\"].shuffle(seed=42).select(range(min(TEST_SUBSET_SIZE, len(dataset[\"test\"]))))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T01:06:25.454656Z","iopub.execute_input":"2025-12-29T01:06:25.454900Z","iopub.status.idle":"2025-12-29T01:06:28.617199Z","shell.execute_reply.started":"2025-12-29T01:06:25.454877Z","shell.execute_reply":"2025-12-29T01:06:28.616612Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bdf1144515a4beeb378ab81f76d4a38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001.parquet:   0%|          | 0.00/3.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cf998db9cb34e818da644fac799b33c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/validation-00000-of-00001.parquet:   0%|          | 0.00/72.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ca28a24034f484b902d4b13999a469f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test-00000-of-00001.parquet:   0%|          | 0.00/148k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1078bcd5f4284806aa8670caea1d686a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"387376c316644e389884dba4dd718623"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"618e62143fff47c4929b8fe54d905012"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91c2a3a9fad049dca4da709675ecbc71"}},"metadata":{}},{"name":"stdout","text":"Original dataset structure:\nDatasetDict({\n    train: Dataset({\n        features: ['idx', 'sentence', 'label'],\n        num_rows: 67349\n    })\n    validation: Dataset({\n        features: ['idx', 'sentence', 'label'],\n        num_rows: 872\n    })\n    test: Dataset({\n        features: ['idx', 'sentence', 'label'],\n        num_rows: 1821\n    })\n})\n\nModified dataset structure:\n  Train: 54552 samples\n  Validation: 6062 samples\n  Test: 6735 samples\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    checkpoint,\n    num_labels=2,\n    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1}\n)\n\n# Move model to GPU\nmodel.to(device)\n\nprint(f\"\\n✓ Loaded model: {checkpoint}\")\nprint(f\"  Parameters: {model.num_parameters():,}\")\nprint(f\"  Device: {next(model.parameters()).device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T01:06:28.618094Z","iopub.execute_input":"2025-12-29T01:06:28.618380Z","iopub.status.idle":"2025-12-29T01:06:31.266041Z","shell.execute_reply.started":"2025-12-29T01:06:28.618342Z","shell.execute_reply":"2025-12-29T01:06:31.265358Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1f753006ec44c9ba8d8a49e068fd610"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8b476065bda4184b4994f33c91a5513"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f68c5041ad345499e54056f3a98424f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95cf2ea4b59b4d8b904ad18997efbddd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df63aadb60524414817c2327618abd5f"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n✓ Loaded model: distilbert-base-uncased\n  Parameters: 66,955,010\n  Device: cuda:0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def tokenize_function(examples):\n    return tokenizer(\n        examples[\"sentence\"],\n        truncation=True,\n        max_length=128,\n    )\n\ntokenized_datasets = dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=[\"sentence\", \"idx\"],\n    desc=\"Tokenizing\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T01:06:31.266898Z","iopub.execute_input":"2025-12-29T01:06:31.267116Z","iopub.status.idle":"2025-12-29T01:06:35.181049Z","shell.execute_reply.started":"2025-12-29T01:06:31.267094Z","shell.execute_reply":"2025-12-29T01:06:35.180166Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Tokenizing:   0%|          | 0/54552 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74aad6ab20ee4f2492562c1d1783828d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing:   0%|          | 0/6062 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64e2444c020e4f608ce487fb8a5e8b16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing:   0%|          | 0/6735 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91a6caa37d8943dc8eb228d89811561e"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"data_collator = DataCollatorWithPadding(\n    tokenizer=tokenizer,\n    padding=True,\n    return_tensors=\"pt\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T01:06:35.181883Z","iopub.execute_input":"2025-12-29T01:06:35.182105Z","iopub.status.idle":"2025-12-29T01:06:35.185708Z","shell.execute_reply.started":"2025-12-29T01:06:35.182083Z","shell.execute_reply":"2025-12-29T01:06:35.185042Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"accuracy_metric = evaluate.load(\"accuracy\")\nf1_metric = evaluate.load(\"f1\")\n\ndef compute_metrics(eval_pred):\n    \"\"\"Compute accuracy and F1 score for evaluation.\"\"\"\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n\n    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"binary\")\n\n    return {\n        \"accuracy\": accuracy[\"accuracy\"],\n        \"f1\": f1[\"f1\"]\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T01:06:35.186838Z","iopub.execute_input":"2025-12-29T01:06:35.187101Z","iopub.status.idle":"2025-12-29T01:06:36.596282Z","shell.execute_reply.started":"2025-12-29T01:06:35.187072Z","shell.execute_reply":"2025-12-29T01:06:36.595491Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b02e75a61874a2c8538a47a8377625a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00f93e29b5c3411dbf1c7d89ecc2bff4"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"/kaggle/working/sst2-distilbert-finetuned\",\n\n    # Training hyperparameters\n    num_train_epochs=5, # increased 3 -> 5\n    learning_rate=2e-5,\n    weight_decay=0.05, # increased 0.01 -> 0.05\n    warmup_ratio=0.1,\n\n    # Batch sizes (optimized for T4 x2)\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=64,\n\n    # Evaluation strategy\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n\n    # Logging\n    logging_dir=\"/kaggle/working/logs\",\n    logging_steps=100,\n    report_to=\"none\",\n\n    # Best model\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",  # 'accuracy' -> 'eval_loss'\n    greater_is_better=False,\n\n    # Reproducibility\n    seed=42,\n\n    # GPU-specific settings\n    fp16=True,                    \n    dataloader_num_workers=2,     # Parallel data loading\n\n    # label smoothing\n    label_smoothing_factor = 0.1, # new added\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T01:06:36.598426Z","iopub.execute_input":"2025-12-29T01:06:36.598710Z","iopub.status.idle":"2025-12-29T01:06:36.638871Z","shell.execute_reply.started":"2025-12-29T01:06:36.598684Z","shell.execute_reply":"2025-12-29T01:06:36.638263Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T01:06:36.639724Z","iopub.execute_input":"2025-12-29T01:06:36.640121Z","iopub.status.idle":"2025-12-29T01:06:36.680078Z","shell.execute_reply.started":"2025-12-29T01:06:36.640086Z","shell.execute_reply":"2025-12-29T01:06:36.679356Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_55/744850133.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 1\neffective_batch_size = training_args.per_device_train_batch_size * num_gpus\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"Starting GPU Training...\")\nprint(f\"Number of GPUs: {num_gpus}\")\nprint(f\"Effective batch size: {effective_batch_size}\")\nprint(\"=\" * 50)\n\ntrain_result = trainer.train()\n\nprint(f\"\\n{'=' * 50}\")\nprint(\"Training Complete!\")\nprint(f\"{'=' * 50}\")\nprint(f\"Total training time: {train_result.metrics['train_runtime']:.2f} seconds\")\nprint(f\"  ({train_result.metrics['train_runtime'] / 60:.2f} minutes)\")\nprint(f\"Samples/second: {train_result.metrics['train_samples_per_second']:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T01:06:36.680934Z","iopub.execute_input":"2025-12-29T01:06:36.681178Z","iopub.status.idle":"2025-12-29T01:19:42.872594Z","shell.execute_reply.started":"2025-12-29T01:06:36.681141Z","shell.execute_reply":"2025-12-29T01:19:42.871961Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nStarting GPU Training...\nNumber of GPUs: 2\nEffective batch size: 64\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4265' max='4265' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4265/4265 13:03, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.335400</td>\n      <td>0.311524</td>\n      <td>0.934345</td>\n      <td>0.940914</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.283400</td>\n      <td>0.287019</td>\n      <td>0.951006</td>\n      <td>0.956975</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.260600</td>\n      <td>0.284998</td>\n      <td>0.952821</td>\n      <td>0.958635</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.247700</td>\n      <td>0.287668</td>\n      <td>0.953316</td>\n      <td>0.958860</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.227400</td>\n      <td>0.290434</td>\n      <td>0.954141</td>\n      <td>0.959499</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nTraining Complete!\n==================================================\nTotal training time: 785.78 seconds\n  (13.10 minutes)\nSamples/second: 347.12\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"print(f\"\\n{'=' * 50}\")\nprint(\"Validation Set Evaluation:\")\nprint(f\"{'=' * 50}\")\n\nval_results = trainer.evaluate(tokenized_datasets[\"validation\"])\nfor key, value in val_results.items():\n    if isinstance(value, float):\n        print(f\"  {key}: {value:.4f}\")\n    else:\n        print(f\"  {key}: {value}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T01:19:42.873702Z","iopub.execute_input":"2025-12-29T01:19:42.874477Z","iopub.status.idle":"2025-12-29T01:19:48.024226Z","shell.execute_reply.started":"2025-12-29T01:19:42.874444Z","shell.execute_reply":"2025-12-29T01:19:48.023480Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nValidation Set Evaluation:\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='101' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [48/48 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"  eval_loss: 0.2850\n  eval_accuracy: 0.9528\n  eval_f1: 0.9586\n  eval_runtime: 5.1396\n  eval_samples_per_second: 1179.4630\n  eval_steps_per_second: 9.3390\n  epoch: 5.0000\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"print(f\"\\n{'=' * 50}\")\nprint(\"Test Set Evaluation:\")\nprint(f\"{'=' * 50}\")\n\ntest_results = trainer.evaluate(tokenized_datasets[\"test\"])\nfor key, value in test_results.items():\n    if isinstance(value, float):\n        print(f\"  {key}: {value:.4f}\")\n    else:\n        print(f\"  {key}: {value}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T01:19:48.025330Z","iopub.execute_input":"2025-12-29T01:19:48.025642Z","iopub.status.idle":"2025-12-29T01:19:53.823158Z","shell.execute_reply.started":"2025-12-29T01:19:48.025610Z","shell.execute_reply":"2025-12-29T01:19:53.822202Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nTest Set Evaluation:\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"  eval_loss: 0.2961\n  eval_accuracy: 0.9471\n  eval_f1: 0.9521\n  eval_runtime: 5.7867\n  eval_samples_per_second: 1163.8850\n  eval_steps_per_second: 9.1590\n  epoch: 5.0000\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"save_path = \"/kaggle/working/sst2-distilbert-final\"\ntrainer.save_model(save_path)\ntokenizer.save_pretrained(save_path)\nprint(f\"\\n✓ Model saved to: {save_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T01:19:53.824586Z","iopub.execute_input":"2025-12-29T01:19:53.825050Z","iopub.status.idle":"2025-12-29T01:19:54.374772Z","shell.execute_reply.started":"2025-12-29T01:19:53.825013Z","shell.execute_reply":"2025-12-29T01:19:54.374091Z"}},"outputs":[{"name":"stdout","text":"\n✓ Model saved to: /kaggle/working/sst2-distilbert-final\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 1\ngpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n\nprint(f\"\\n{'=' * 50}\")\nprint(\"TRAINING SUMMARY\")\nprint(f\"{'=' * 50}\")\nprint(f\"  Model: {checkpoint}\")\nprint(f\"  Accelerator: {num_gpus}x {gpu_name}\")\nprint(f\"  Mode: {'TEST' if TEST_MODE else 'FULL'}\")\nprint(f\"  Training samples: {len(tokenized_datasets['train'])}\")\nprint(f\"  Effective batch size: {training_args.per_device_train_batch_size * num_gpus}\")\nprint(f\"  Validation Accuracy: {val_results['eval_accuracy']:.4f}\")\nprint(f\"  Test Accuracy: {test_results['eval_accuracy']:.4f}\")\nprint(f\"  Total Time: {train_result.metrics['train_runtime'] / 60:.2f} minutes\")\nprint(f\"{'=' * 50}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T01:19:54.375637Z","iopub.execute_input":"2025-12-29T01:19:54.375880Z","iopub.status.idle":"2025-12-29T01:19:54.999697Z","shell.execute_reply.started":"2025-12-29T01:19:54.375857Z","shell.execute_reply":"2025-12-29T01:19:54.999147Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nTRAINING SUMMARY\n==================================================\n  Model: distilbert-base-uncased\n  Accelerator: 2x Tesla T4\n  Mode: FULL\n  Training samples: 54552\n  Effective batch size: 64\n  Validation Accuracy: 0.9528\n  Test Accuracy: 0.9471\n  Total Time: 13.10 minutes\n==================================================\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"### Run Evaluation\n1st Run: \n\nCRITICAL ERROR - Used validation data for model finetuning and test evaluation. Results were skewed.\n\nChanges - Adjusted epoch amount, changed evaluation metric from accuracy to eval_loss, added label smoothing to address overfitting","metadata":{}},{"cell_type":"markdown","source":"2nd Run: \n\nCRITICAL ERROR - best model selected was the one with largest eval_loss, not the least eval_loss\n\nChanges - greater_is_better value = False","metadata":{}},{"cell_type":"markdown","source":"Final Run: \n\nModel: distilbert-base-uncased\n\nAccelerator: 2x Tesla T4\n\nMode: FULL\n\nTraining samples: 54552\n\nEffective batch size: 64\n\nValidation Accuracy: 0.9528\n\nTest Accuracy: 0.9471\n\nTotal Time: 13.10 minutes\n","metadata":{}},{"cell_type":"markdown","source":"### Results\n\nStopping finetuning here. Test accuracy over 6000 test data is 94.71% Improvements possible, but I would rather compare initial results to initial results for traditional sentiment analysis method and LLM call method","metadata":{}}]}